{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">LDAによるニュース分類</font>\n",
    "\n",
    "Source: https://dev.classmethod.jp/machine-learning/2017ad_20171221_lda_python/\n",
    "\n",
    "機械学習_潜在意味解析_pythonで実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">データのロード</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.mac.hardware', 'comp.windows.x', 'rec.sport.baseball', 'rec.sport.hockey']\n",
      "2368\n",
      "1576\n",
      "From: pkeenan@s.psych.uiuc.edu (Patricia Keenan)\n",
      "Subject: Re: Quick easy question!\n",
      "Organization: UIUC Department of Psychology\n",
      "Lines: 16\n",
      "\n",
      "rauser@fraser.sfu.ca (Richard John Rauser) writes:\n",
      "\n",
      "\n",
      "\n",
      ">   Here's an easy question for someone who knows nothing about baseball...\n",
      "\n",
      ">   What city do the California Angels play out of?\n",
      "\n",
      "   Anaheim.\n",
      "\n",
      ">-- \n",
      ">Richard J. Rauser        \"You have no idea what you're doing.\"\n",
      ">rauser@sfu.ca            \"Oh, don't worry about that. We're professional\n",
      ">WNI                          outlaws - we do this for a living.\"\n",
      ">-----------------\n",
      ">\"Remember, no matter where you go, there you are.\" -Dr.Banzai\n",
      "\n",
      "rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "'''1.必要なモジュールとデータセットの準備\n",
    "sklearnに用意されている「ニュース」のデータセットを利用します\n",
    "'''\n",
    " \n",
    "# モジュールのインポート\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    " \n",
    "'''\n",
    "sklearnに用意されている「ニュース」のデータセットを利用します。\n",
    "'''\n",
    " \n",
    "# カテゴリーの絞り込み\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey', 'comp.sys.mac.hardware', 'comp.windows.x']\n",
    " \n",
    "# トレーニング用、検証用のデータセット作成\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)\n",
    " \n",
    "# データの中身確認\n",
    "print(twenty_train.target_names) # カテゴリの確認(どんなカテゴリがあるのか)\n",
    "print (len(twenty_train.data)) # ニュースデータが何本あるのか（2,368本のニュースデータ）\n",
    "print (len(twenty_test.data)) # ニュースデータが何本あるのか（1,576本のニュースデータ）\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\"))) #データの内容を確認(1個の文書の中身を確認)\n",
    "print(twenty_train.target_names[twenty_train.target[0]]) #データのカテゴリーを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">データの前処理</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idfの計算結果の概要\n",
      ":<2368x7562 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 159495 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "最初の３０個の単語\n",
      ":['00', '000', '0062', '00mbstultz', '01', '02', '0200', '0223', '024222', '03', '030', '04', '040', '0400', '05', '051', '06', '065', '0666', '07', '08', '09', '0ha', '100', '1000', '101', '1010', '102', '1020', '1024']\n",
      "\n",
      "500-530\n",
      ":['636', '637', '64', '640', '643', '645', '647', '649', '64k', '65', '650', '653', '655', '656', '657', '66', '660', '664', '666', '667', '66mhz', '67', '672', '675', '678', '68', '680', '68000', '68020', '68030']\n",
      "\n",
      "1000-1030\n",
      ":['ask', 'asked', 'asking', 'asks', 'aspect', 'aspects', 'ass', 'assat', 'assembled', 'assessment', 'asshole', 'assholes', 'assist', 'assistance', 'assistant', 'assists', 'associated', 'associates', 'association', 'assume', 'assumed', 'assumes', 'assuming', 'ast', 'astro', 'astronomy', 'astros', 'asynchronous', 'atd', 'athena']\n"
     ]
    }
   ],
   "source": [
    "'''2.前処理\n",
    "前処理で実施したものは下記の４点。\n",
    "・文字を全て小文字化\n",
    "・stop words削除\n",
    "・全文書の１割以上の文書に出現する単語の削除\n",
    "・5回以上出現する単語のみ\n",
    "'''\n",
    " \n",
    "# tf-idf計算した時の単語数を確認してみる\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "tfidf_vec = TfidfVectorizer(lowercase=True, stop_words='english', max_df = 0.1, min_df = 5).fit(twenty_train.data) # \n",
    "X_train = tfidf_vec.transform(twenty_train.data) \n",
    "X_test = tfidf_vec.transform(twenty_test.data) \n",
    " \n",
    "print('tf-idfの計算結果の概要\\n:{}'.format(repr(X_train))) # 2,368行7,562列の疎行列\n",
    " \n",
    "# 各単語の中身を確認する\n",
    "feature_names = tfidf_vec.get_feature_names()\n",
    "print('\\n最初の３０個の単語\\n:{}'.format(feature_names[:30]))\n",
    "print('\\n500-530\\n:{}'.format(feature_names[500:530]))\n",
    "print('\\n1000-1030\\n:{}'.format(feature_names[1000:1030]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">モデルの生成と評価(注意！実行にかなり時間がかかる)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トピック数:2,反復回数:50回,処理時間:0.38分,exp(-1. * log-likelihood per word)(perplexity):11768.51\n",
      "トピック数:2,反復回数:100回,処理時間:0.62分,exp(-1. * log-likelihood per word)(perplexity):11768.57\n",
      "トピック数:2,反復回数:300回,処理時間:1.66分,exp(-1. * log-likelihood per word)(perplexity):11768.57\n",
      "トピック数:2,反復回数:500回,処理時間:2.61分,exp(-1. * log-likelihood per word)(perplexity):11768.57\n",
      "トピック数:3,反復回数:50回,処理時間:0.37分,exp(-1. * log-likelihood per word)(perplexity):17218.32\n",
      "トピック数:3,反復回数:100回,処理時間:0.62分,exp(-1. * log-likelihood per word)(perplexity):17215.31\n",
      "トピック数:3,反復回数:300回,処理時間:1.66分,exp(-1. * log-likelihood per word)(perplexity):17210.34\n",
      "トピック数:3,反復回数:500回,処理時間:2.64分,exp(-1. * log-likelihood per word)(perplexity):17210.34\n",
      "トピック数:4,反復回数:50回,処理時間:0.36分,exp(-1. * log-likelihood per word)(perplexity):19769.88\n",
      "トピック数:4,反復回数:100回,処理時間:0.62分,exp(-1. * log-likelihood per word)(perplexity):19660.65\n",
      "トピック数:4,反復回数:300回,処理時間:1.51分,exp(-1. * log-likelihood per word)(perplexity):19552.25\n",
      "トピック数:4,反復回数:500回,処理時間:2.57分,exp(-1. * log-likelihood per word)(perplexity):19552.25\n",
      "トピック数:5,反復回数:50回,処理時間:0.41分,exp(-1. * log-likelihood per word)(perplexity):27679.2\n",
      "トピック数:5,反復回数:100回,処理時間:0.62分,exp(-1. * log-likelihood per word)(perplexity):27635.09\n",
      "トピック数:5,反復回数:300回,処理時間:1.61分,exp(-1. * log-likelihood per word)(perplexity):27582.52\n",
      "トピック数:5,反復回数:500回,処理時間:2.63分,exp(-1. * log-likelihood per word)(perplexity):27583.2\n",
      "トピック数:6,反復回数:50回,処理時間:0.35分,exp(-1. * log-likelihood per word)(perplexity):28876.65\n",
      "トピック数:6,反復回数:100回,処理時間:0.59分,exp(-1. * log-likelihood per word)(perplexity):28715.61\n",
      "トピック数:6,反復回数:300回,処理時間:1.64分,exp(-1. * log-likelihood per word)(perplexity):28685.25\n",
      "トピック数:6,反復回数:500回,処理時間:2.5分,exp(-1. * log-likelihood per word)(perplexity):28685.25\n",
      "トピック数:10,反復回数:50回,処理時間:0.36分,exp(-1. * log-likelihood per word)(perplexity):71089.24\n",
      "トピック数:10,反復回数:100回,処理時間:0.63分,exp(-1. * log-likelihood per word)(perplexity):70729.3\n",
      "トピック数:10,反復回数:300回,処理時間:1.64分,exp(-1. * log-likelihood per word)(perplexity):70563.57\n",
      "トピック数:10,反復回数:500回,処理時間:2.6分,exp(-1. * log-likelihood per word)(perplexity):70688.6\n"
     ]
    }
   ],
   "source": [
    "''' 3.モデル生成と評価\n",
    "「perplexity」を評価指標とします。\n",
    "パラメータは、「トピック数」と「反復回数」を指定します。\n",
    "本来は「反復回数」は推移を確認したほうがいいとは思うのですが...\n",
    "'''\n",
    " \n",
    "# 3-1.パラメータの調整\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import time\n",
    "n_comp = [2, 3, 4, 5, 6,10] # トピック数の指定\n",
    "max_iter = [50, 100, 300, 500] # 反復回数の指定\n",
    " \n",
    "for comp in n_comp:\n",
    "    for max_i in max_iter:\n",
    " \n",
    "        # 実行時間計測開始\n",
    "        start = time.time()\n",
    " \n",
    "        lda =LatentDirichletAllocation(n_components=comp,  max_iter=max_i, learning_method='batch', random_state=0, n_jobs=-1)\n",
    "        lda_fit = lda.fit(X_train)\n",
    "        lda_perp = lda.perplexity(X_test)\n",
    " \n",
    "        # 実行時間計測終了\n",
    "        elapsed_time = time.time() - start \n",
    " \n",
    "        # 結果の表示\n",
    "        print(\"トピック数:{0},反復回数:{1}回,処理時間:{2}分,exp(-1. * log-likelihood per word)(perplexity):{3}\".format(comp, max_i, round(elapsed_time/60, 2), round(lda_perp, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">トピックの確認と可視化</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3.モデル生成と評価\n",
    "上記の結果から、トピック数を2とします。\n",
    "また、反復回数は100回くらいで良さそうですね\n",
    "'''\n",
    " \n",
    "# 3-2.先ほど求めたパラメータに応じたモデルの生成\n",
    " \n",
    "# データの用意（トレーニング用、検証用と分けずに全セット）\n",
    "twenty_train = fetch_20newsgroups(categories=categories, shuffle=True, random_state=42)\n",
    "X = tfidf_vec.transform(twenty_train.data) \n",
    " \n",
    "# クラスの生成\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda =LatentDirichletAllocation(n_components=2,  max_iter=100, learning_method='batch', random_state=0, n_jobs=-1)\n",
    " \n",
    "# モデルの生成\n",
    "lda.fit(X)\n",
    "lda_X = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: tedward@cs', 'cornell', 'edu (Edward [Ted] Fischer)\\nSubject: Old Predictions to laugh at', '', '', '\\nSummary: LONG!\\nOrganization: Cornell Univ', ' CS Dept, Ithaca NY 14853\\nLines: 404\\n\\n\\nOops!  I came across this file from last year', '  Thought you might\\nenjoy some of these thoughts', '  The predictions were made on the\\ndate indicated', '  They are largely out of order', '\\n\\n------------------------------------------------------------------------------\\n\\nJune 11, 1992\\ntedward@cs', 'cornell', 'edu (ME!)\\n>What have I done?  I computed the \"expected winning percentage\" for\\n>each team from their OBP, total bases, and runs allowed', '  I use the\\n>basic RC formula and the pythagorean projection', '  I then compare this\\n>with their actual winning percentage', '  All stats through June 7', '\\n>\\n>Team           OBP    TB    RA     W     L    XWP  Diff\\n>baltimore    0', '351   768   199    33    21  0', '647   -36\\n>boston       0', '334   580   176    26    25  0', '548   -38\\n>toronto      0', '319   750   221    34    22  0', '540    68\\n>new york     0', '327   759   237    28    26  0', '523    -5\\n>milwaukee    0', '325   692   226    28    25  0', '498    31\\n>detroit      0', '328   782   285    24    31  0', '448   -11\\n>cleveland    0', '316   688   274    22    34  0']\n",
      "\n",
      "------------------------------\n",
      "['From: dxf12@po', 'CWRU', 'Edu (Douglas Fowler)\\nSubject: (ATAS) N', 'L', ' games 8/2-8/5 & standings of all\\nArticle-I', 'D', ': usenet', '1pqf84$caf\\nOrganization: Case Western Reserve University, Cleveland, Ohio (USA)\\nLines: 237\\nNNTP-Posting-Host: slc12', 'ins', 'cwru', 'edu\\n\\n\\n     Philadelphia at Chicago:  Teams tied for 1st after Sunday\\n     Dick Redding battled Chet Brewer in the first game of a dramatic four\\ngame series', '  One Friday, one Saturday, and a good-old Sunday doubleheader', '\\n\"What could be better,\" declared Ernie Banks', '  Perhaps the fact that the Cubs\\nare challenging?\\n     \"It\\'s pitching, it\\'s always been pitching that we\\'ve lacked,\" announced\\nRyne Sandberg', '  \"If we can get by Brewer, then beat Carlton, Alexander, or\\nBunning - preferrably 2 of the last three - we\\'ll know we might be able to\\nwin', '\\n     \"Lord, I hope we pull it off', '\"\\n     The Phils scored once in the top of the first; Richie Ashburn singled, Pete\\nRose followed with a hit, sending Ashburn around second', '  Kiki Cuyler cut\\nthe ball off in left center, and threw a bullet in to Ernie Banks, who threw\\nto Ron Santo to get Ashburn at third', '  Rose went to second on the play', '\\n     Christobel Torrienti lifted a long fly to center, moving Pete Rose to\\nthird', '  Schmidt was walked - the Cubs were absolutely refusing to let him\\nbeat them', '  Both Torrienti and Schmidt will likely draw 130-150 walks this\\nyear', '  Chuck Klein is starting to hit very well, and he lashed a double into a\\ngap in right-center', '  \"Cool Papa\" Bell\\'s speed allowed him to cut the ball off\\nand prevent Schmidt from scoring', '  Nellie Fox was walked, and Bob Boone\\ngrounded out to second, ending the threat', '  \\n     \"Teams are starting to realize that you don\\'t have to pitch to Schmidt or\\nTorrienti, and that is lowering their run total', \"  It puts a lot of pressure on\\nKlein and Dick Allen (who platoons with Chuck Klein and occasionally spells\\nRose at first), and it's a credit to the Phillies that they've been able to\\nsustain their pace\", '  The picthers have slumped at times', '\"  So came the\\nanalysis from Frank Chance', \"\\n     The Cubs got that run back when Bell bunted for a hit, Thomas' grounder\\nmoved him to second, and - after Sandberg made out - Billy Williams singled\\nhome a run\"]\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "''' 4.トピックの確認\n",
    "各トピックに実際にどのような単語、文書が割り当てられているのかを確認します。\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# 4-2.各トピックごとにどんな文書が割り当てられているか\n",
    " \n",
    "# トピック０\n",
    "topic_0 = np.argsort(lda_X[:, 0])[::-1] # このトピックでもっとも重要度が高い順にソート\n",
    " \n",
    "for i in range(0,2):\n",
    "    print(twenty_train.data[topic_0[i]].split('.')[:30])\n",
    "    print('\\n' + '------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">テストデータによる評価</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83430136 0.16569864]\n",
      " [0.83230705 0.16769295]]\n"
     ]
    }
   ],
   "source": [
    "'''5.テストデータを入れて確認\n",
    "yahooニュースをgoogle翻訳で英語にしたものを入れてみた\n",
    "'''\n",
    " \n",
    "# 5-1.大谷選手がエンゼルス入団を決めたニュースでテスト\n",
    " \n",
    "test = [\n",
    "'Nippon Ham\\'s Shohei Otani pitcher (23) who was aiming for a major transfer in the posting system decided to contract with Angels. On August 8, agent Mr. Barzero announced. In addition, Angels also announced the following statement.',\n",
    "'The U.S. major league, Angels announced that the shoulder number of Shohei Otani pitcher (23) whose entry was decided will be \"17\".', \n",
    "]\n",
    " \n",
    " \n",
    "X_test = tfidf_vec.transform(test) \n",
    "lda_test = lda.transform(X_test)\n",
    "print(lda_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36815481 0.63184519]\n",
      " [0.12606953 0.87393047]]\n"
     ]
    }
   ],
   "source": [
    "'''5.テストデータを入れて確認\n",
    "yahooニュースをgoogle翻訳で英語にしたものを入れてみた\n",
    "'''\n",
    " \n",
    "# 5-2.macpro発売、macへの不正ログインのニュース\n",
    " \n",
    "test=[\n",
    "'Apple announced on December 12 (local time) that December 14 will release a new desktop iMac Pro that will be \"the most powerful Mac ever\". Although the selling price in Japan is unpublished, it is 4,999 dollars (about 570,000 yen) in the United States.',\n",
    "'Developer Lemi Orhan Ergin discovered a vulnerability in macOS High Sierra 10.13.1. When you click on the lock button from \"User and group\" in the system environment setting and enter \"user name\" and password in order to unlock the preference setting, enter \"root\" for the user name, enter the password Even without it, the lock is released.'\n",
    "]\n",
    " \n",
    " \n",
    "X_test = tfidf_vec.transform(test) \n",
    "lda_test = lda.transform(X_test)\n",
    "print(lda_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
